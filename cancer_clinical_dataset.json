# app.py

import streamlit as st
import json
import pandas as pd
from datetime import datetime
from collections import defaultdict
import random
import os
import re
from nltk.stem import PorterStemmer
from nltk.corpus import stopwords
from Levenshtein import distance as levenshtein_distance # For fuzzy matching

# --- NLTK Data Download (Run once if not already downloaded) ---
# This block ensures that necessary NLTK data (stopwords) are available.
# It will download them if they are not found, which might take a moment on first run.
try:
    _ = stopwords.words('english')
    _ = PorterStemmer()
except LookupError:
    import nltk
    st.warning("Downloading NLTK data (stopwords). This will only happen once.")
    nltk.download('stopwords')
    st.success("NLTK data downloaded. Please rerun the app if it doesn't automatically refresh.")
    # You might need to manually rerun the app after the download completes if it doesn't auto-refresh.

# --- Session State Initialization ---
# Initialize all necessary variables in Streamlit's session state.
# Session state persists values across reruns of the app.
if 'search_history' not in st.session_state:
    st.session_state.search_history = []
if 'current_query' not in st.session_state:
    st.session_state.current_query = ""
if 'show_home' not in st.session_state:
    st.session_state.show_home = True
if 'min_score' not in st.session_state:
    st.session_state.min_score = 1
if 'keyword_filters' not in st.session_state:
    st.session_state.keyword_filters = []
if 'cancer_type_filter' not in st.session_state:
    st.session_state.cancer_type_filter = []
if 'gene_filter' not in st.session_state:
    st.session_state.gene_filter = []
if 'suggestions' not in st.session_state:
    st.session_state.suggestions = []
if 'cancer_types' not in st.session_state:
    st.session_state.cancer_types = []
if 'genes' not in st.session_state:
    st.session_state.genes = []

# --- Constants ---
DATA_FILE = "cancer_clinical_dataset.json" # Name of your JSON dataset file
HISTORY_FILE = "search_history.json"      # File to store search history
LEVENSHTEIN_THRESHOLD = 2                 # Max Levenshtein distance for fuzzy matching (tune as needed)

# --- NLP Tools Initialization ---
stemmer = PorterStemmer()                  # Initialize the Porter Stemmer
stop_words = set(stopwords.words('english')) # Load English stop words into a set for fast lookup

# --- Utility Functions for Text Preprocessing ---

def preprocess_text(text):
    """
    Converts text to lowercase, removes punctuation, stems words, and removes stop words.
    This function is used for general text cleaning.
    """
    text = str(text).lower()
    text = re.sub(r'[^\w\s]', '', text) # Remove punctuation (keep alphanumeric and spaces)
    words = [stemmer.stem(word) for word in text.split() if word.strip() and word not in stop_words]
    return " ".join(words)

def get_clean_words(text):
    """
    Returns a set of stemmed, non-stop words from a given text.
    Useful for creating word sets for comparison.
    """
    text = str(text).lower()
    text = re.sub(r'[^\w\s]', '', text)
    return set(stemmer.stem(word) for word in text.split() if word.strip() and word not in stop_words)

# --- Search History Management ---

def load_search_history():
    """Loads search history from a JSON file."""
    if os.path.exists(HISTORY_FILE):
        try:
            with open(HISTORY_FILE, "r", encoding="utf-8") as f:
                return json.load(f)
        except Exception as e:
            st.warning(f"Could not load search history: {e}")
            return []
    return []

def save_search_history():
    """Saves current search history to a JSON file."""
    try:
        with open(HISTORY_FILE, "w", encoding="utf-8") as f:
            json.dump(st.session_state.search_history, f)
    except Exception as e:
        st.warning(f"Could not save search history: {e}")

# --- Data Loading and Indexing ---

@st.cache_data # Cache the data loading and indexing for performance
def load_and_index_data():
    """
    Loads the JSON dataset, cleans it, creates a word index for searching,
    and extracts unique cancer types and genes.
    """
    try:
        if not os.path.exists(DATA_FILE):
            st.error(f"Data file '{DATA_FILE}' not found. Please ensure it's in the same directory.")
            return None, None, [], [], []

        with open(DATA_FILE, "r", encoding="utf-8") as f:
            raw_data = json.load(f)
        
        clean_data = []
        word_index = defaultdict(list) # Maps stemmed words to a list of document IDs
        cancer_types = set()
        genes = set()
        all_prompts = [] # Used for generating random suggestions

        for idx, entry in enumerate(raw_data):
            # Ensure entry is a dictionary and has 'prompt' and 'completion' fields
            if isinstance(entry, dict) and "prompt" in entry and "completion" in entry:
                prompt = str(entry["prompt"]).strip()
                completion = str(entry["completion"]).strip()
                
                # Extract and clean metadata (cancer_type, genes)
                entry_cancer_types = [ct.strip() for ct in str(entry.get("cancer_type", "")).split(",") if ct.strip()]
                cancer_types.update(entry_cancer_types)
                
                entry_genes = [g.strip() for g in str(entry.get("genes", "")).split(",") if g.strip()]
                genes.update(entry_genes)
                
                # Store cleaned entry in our main data list
                clean_entry = {
                    "prompt": prompt,
                    "completion": completion,
                    "cancer_type": ", ".join(entry_cancer_types) if entry_cancer_types else "",
                    "genes": ", ".join(entry_genes) if entry_genes else ""
                }
                clean_data.append(clean_entry)
                all_prompts.append(prompt)

                # Index stemmed words from both prompt and completion for search
                processed_prompt_words = get_clean_words(prompt)
                processed_completion_words = get_clean_words(completion)

                for word in processed_prompt_words.union(processed_completion_words):
                    if len(word) > 2: # Only index words longer than 2 characters
                        word_index[word].append(idx)

        if not clean_data:
            st.error("No valid Q&A pairs found in the dataset. Please check your JSON file.")
            return None, None, [], [], []
        
        # Generate random suggestions from the prompts
        random_suggestions = random.sample(all_prompts, min(10, len(all_prompts))) if all_prompts else []
        
        # Return processed data, index, and metadata
        return clean_data, word_index, sorted(list(cancer_types)), sorted(list(genes)), random_suggestions
    
    except json.JSONDecodeError:
        st.error(f"Error: '{DATA_FILE}' is not a valid JSON file. Please check its format.")
        return None, None, [], [], []
    except Exception as e:
        st.error(f"An unexpected error occurred while loading data: {str(e)}")
        return None, None, [], [], []

# --- Search Logic ---

def keyword_search(query, dataset, word_index):
    """
    Performs a keyword search on the dataset, incorporating stemming,
    stop word removal, exact matching, and fuzzy matching (Levenshtein distance).
    Results are scored and ranked.
    """
    if not query or not dataset:
        return []
    
    # Preprocess the query words
    query_words_stemmed = get_clean_words(query)
    
    # Dictionary to store match information for each document
    # Includes score, matched keywords, and counts for prompt/completion matches
    doc_matches = defaultdict(lambda: {"score": 0, "matched_keywords": set(), "prompt_matches": 0, "completion_matches": 0})
    
    # Iterate through each document in the dataset
    for doc_id, entry in enumerate(dataset):
        # Get clean (stemmed, no stop words) words from the document's prompt and completion
        processed_prompt_words = get_clean_words(entry["prompt"])
        processed_completion_words = get_clean_words(entry["completion"])
        
        current_matched_keywords = set() # Keywords that actually matched in this specific document

        # Compare each query word with words in the document
        for q_word in query_words_stemmed:
            # 1. Exact Match (higher score)
            if q_word in processed_prompt_words:
                doc_matches[doc_id]["prompt_matches"] += 1
                current_matched_keywords.add(q_word)
            if q_word in processed_completion_words:
                doc_matches[doc_id]["completion_matches"] += 1
                current_matched_keywords.add(q_word)

            # 2. Fuzzy Match (Levenshtein distance - partial score)
            # Check for fuzzy matches in prompt
            found_fuzzy_prompt = False
            for p_word in processed_prompt_words:
                if levenshtein_distance(q_word, p_word) <= LEVENSHTEIN_THRESHOLD:
                    doc_matches[doc_id]["prompt_matches"] += 0.5 # Partial score for fuzzy match
                    current_matched_keywords.add(p_word) # Add the actual word from the document
                    found_fuzzy_prompt = True
                    break # Move to next query word once a fuzzy match is found in prompt

            # Check for fuzzy matches in completion (only if no fuzzy match found in prompt for this q_word)
            if not found_fuzzy_prompt: # Avoid double counting if a word is fuzzy matched in both
                for c_word in processed_completion_words:
                    if levenshtein_distance(q_word, c_word) <= LEVENSHTEIN_THRESHOLD:
                        doc_matches[doc_id]["completion_matches"] += 0.25 # Smaller partial score
                        current_matched_keywords.add(c_word) # Add the actual word from the document
                        break # Move to next query word

        # Calculate total score with weights: Prompt matches are weighted higher
        doc_matches[doc_id]["score"] = (doc_matches[doc_id]["prompt_matches"] * 2) + doc_matches[doc_id]["completion_matches"]
        # Update the set of matched keywords for this document
        doc_matches[doc_id]["matched_keywords"].update(current_matched_keywords)

    ranked_results = []
    for doc_id, match_info in doc_matches.items():
        if match_info["score"] > 0: # Only include documents with a non-zero score
            ranked_results.append({
                "entry": dataset[doc_id],
                "score": match_info["score"],
                "prompt_matches": match_info["prompt_matches"],
                "completion_matches": match_info["completion_matches"],
                "matched_keywords": match_info["matched_keywords"]
            })

    # Sort results by score in descending order
    ranked_results.sort(key=lambda x: x["score"], reverse=True)
    return ranked_results

# --- Filtering Logic ---

def filter_results(results, min_score, keyword_filters, cancer_types, genes):
    """
    Filters a list of search results based on minimum score, keyword filters,
    cancer type filters, and gene filters.
    """
    filtered = []
    
    # Preprocess filter keywords for consistent comparison (stemming + stop words)
    stemmed_keyword_filters = {stemmer.stem(kw.lower()) for kw in keyword_filters if kw.strip()}
    
    for result in results:
        entry = result["entry"]
        
        # 1. Apply Minimum Score Filter
        if result["score"] < min_score:
            continue
        
        # 2. Apply Keyword Filters (fuzzy matching for filters too)
        if stemmed_keyword_filters:
            matched_by_filter = False
            # Check if any of the filter keywords (stemmed) are present in the result's matched keywords
            # or if they are fuzzy matches to the result's matched keywords
            for f_kw in stemmed_keyword_filters:
                if any(levenshtein_distance(f_kw, matched_kw) <= LEVENSHTEIN_THRESHOLD for matched_kw in result["matched_keywords"]):
                    matched_by_filter = True
                    break
            if not matched_by_filter:
                continue
        
        # 3. Apply Cancer Type Filter
        if cancer_types and entry["cancer_type"]:
            entry_types = {ct.strip() for ct in entry["cancer_type"].split(",")}
            if not any(ct in entry_types for ct in cancer_types):
                continue
        
        # 4. Apply Gene Filter
        if genes and entry["genes"]:
            entry_genes = {g.strip() for g in entry["genes"].split(",")}
            if not any(g in entry_genes for g in genes):
                continue
        
        filtered.append(result)
    return filtered

# --- UI Helper for Keyword Highlighting ---

def highlight_keywords(text, keywords):
    """
    Highlights (bolds) specified keywords within a given text using Markdown.
    It handles case-insensitivity and ensures whole word matches.
    """
    highlighted_text = text
    # Sort keywords by length descending to prevent shorter words from being bolded within longer ones
    sorted_keywords = sorted(list(keywords), key=len, reverse=True)
    
    for keyword in sorted_keywords:
        # Create a regex pattern to match the whole word (case-insensitive)
        # re.escape handles special characters in keywords
        pattern = r'\b(' + re.escape(keyword) + r')\b'
        # Replace matched word with bolded version
        highlighted_text = re.sub(pattern, r'**\1**', highlighted_text, flags=re.IGNORECASE)
    return highlighted_text

# --- Streamlit Page Layouts ---

def show_home():
    """Defines the layout and behavior of the application's home page."""
    st.title("üß¨ Precision Cancer Clinical Search")
    st.markdown("""
    **Find precise answers about cancer treatments and clinical trials** üîç
    This tool helps researchers access structured clinical trial information.
    """)

    # Display random suggestions to guide the user
    if st.session_state.suggestions:
        st.markdown("---")
        st.markdown("**üí° Try these sample questions:**")
        cols = st.columns(2) # Display suggestions in two columns
        for i, suggestion in enumerate(st.session_state.suggestions[:6]): # Limit to 6 suggestions
            with cols[i % 2]: # Distribute buttons across columns
                # Create a button for each suggestion
                if st.button(suggestion[:70] + "..." if len(suggestion) > 70 else suggestion, 
                             key=f"suggestion_{i}"):
                    st.session_state.current_query = suggestion # Set query and navigate to results
                    st.session_state.show_home = False
                    st.rerun() # Rerun the app to switch to results page
        st.markdown("---")

    # Search input form
    with st.form("search_form"):
        query = st.text_input(
            "Search clinical questions:",
            value=st.session_state.current_query, # Pre-fill with current query if any
            placeholder="e.g., What is the response rate for atezolizumab in PD-L1 high NSCLC patients?",
            help="Enter your clinical question or keywords"
        )

        # Submit button for the search form
        if st.form_submit_button("Search", type="primary"):
            if query.strip(): # Only proceed if query is not empty
                st.session_state.current_query = query.strip()
                st.session_state.show_home = False
                # Add current query to search history (most recent first)
                st.session_state.search_history.insert(0, {
                    "query": st.session_state.current_query,
                    "timestamp": datetime.now().isoformat()
                })
                # Keep history limited to a reasonable number (e.g., 20)
                st.session_state.search_history = st.session_state.search_history[:20]
                save_search_history() # Save history to file
                st.rerun() # Rerun to show results
            else:
                st.warning("Please enter a search query.")

def show_results():
    """Defines the layout and behavior of the search results page."""
    # Button to go back to the home page
    if st.button("‚Üê Back to Home"):
        st.session_state.show_home = True
        st.rerun()

    st.title("üîç Search Results")
    
    # --- Filters Sidebar ---
    with st.sidebar:
        st.subheader("üîé Refine Results")
        
        # Slider for Minimum Match Score
        st.session_state.min_score = st.slider(
            "Minimum Match Score",
            min_value=0,
            max_value=20, # Max value adjusted for potential fuzzy scores
            value=st.session_state.min_score,
            help="Higher scores mean more keywords matched (exact and fuzzy)"
        )
        
        # Input for adding keyword filters
        st.markdown("**Filter by keywords:**")
        new_keyword = st.text_input("Add keyword filter", key="new_keyword_filter_input")
        add_keyword_button = st.button("Add Keyword Filter", key="add_keyword_filter_button")

        if add_keyword_button and new_keyword.strip():
            # Add keyword only if not already present (case-insensitive)
            if new_keyword.lower() not in [k.lower() for k in st.session_state.keyword_filters]:
                st.session_state.keyword_filters.append(new_keyword.strip())
                st.rerun() # Rerun to apply filter

        # Multiselect for Cancer Type filters
        if st.session_state.cancer_types:
            st.markdown("---")
            st.markdown("**Filter by cancer type:**")
            selected_cancers = st.multiselect(
                "Select cancer types",
                st.session_state.cancer_types,
                default=st.session_state.cancer_type_filter,
                key="cancer_type_select"
            )
            # Update session state and rerun if selection changes
            if selected_cancers != st.session_state.cancer_type_filter:
                st.session_state.cancer_type_filter = selected_cancers
                st.rerun()
        
        # Multiselect for Gene filters
        if st.session_state.genes:
            st.markdown("---")
            st.markdown("**Filter by genes:**")
            selected_genes = st.multiselect(
                "Select genes",
                st.session_state.genes,
                default=st.session_state.gene_filter,
                key="gene_select"
            )
            # Update session state and rerun if selection changes
            if selected_genes != st.session_state.gene_filter:
                st.session_state.gene_filter = selected_genes
                st.rerun()
        
        st.markdown("---")
        # Display active filters and provide 'X' buttons to remove them
        if st.session_state.keyword_filters or st.session_state.cancer_type_filter or st.session_state.gene_filter:
            st.markdown("**Active Filters:**")
            filters_to_remove = []

            # Display and allow removal of keyword filters
            for i, f_kw in enumerate(st.session_state.keyword_filters):
                if st.button(f"‚ùå Keyword: {f_kw}", key=f"remove_kw_{i}"):
                    filters_to_remove.append(("keyword", f_kw))
            
            # Display and allow removal of cancer type filters
            for i, f_ct in enumerate(st.session_state.cancer_type_filter):
                if st.button(f"‚ùå Cancer Type: {f_ct}", key=f"remove_ct_{i}"):
                    filters_to_remove.append(("cancer_type", f_ct))

            # Display and allow removal of gene filters
            for i, f_gene in enumerate(st.session_state.gene_filter):
                if st.button(f"‚ùå Gene: {f_gene}", key=f"remove_gene_{i}"):
                    filters_to_remove.append(("gene", f_gene))

            # Apply removals if any 'X' button was clicked
            if filters_to_remove:
                for f_type, f_value in filters_to_remove:
                    if f_type == "keyword":
                        st.session_state.keyword_filters.remove(f_value)
                    elif f_type == "cancer_type":
                        st.session_state.cancer_type_filter.remove(f_value)
                    elif f_type == "gene":
                        st.session_state.gene_filter.remove(f_value)
                st.rerun() # Rerun to update filters

        # Button to clear all active filters
        if st.button("Clear All Filters", type="secondary"):
            st.session_state.keyword_filters = []
            st.session_state.cancer_type_filter = []
            st.session_state.gene_filter = []
            st.session_state.min_score = 1 # Reset min score
            st.rerun() # Rerun to clear filters

    # --- Main Results Area ---

    # Load data and index (cached for efficiency)
    data, word_index, cancer_types, genes, _ = load_and_index_data()
    if data is None: # Handle case where data loading failed
        return

    # Store cancer types and genes in session state if not already populated
    # This ensures multiselects have options even if data was loaded in a previous session
    if not st.session_state.cancer_types and cancer_types:
        st.session_state.cancer_types = cancer_types
    if not st.session_state.genes and genes:
        st.session_state.genes = genes

    # Display current search query
    st.markdown(f"**Current Search Query:** **`{st.session_state.current_query}`**")

    # Perform search and filtering with a spinner for user feedback
    with st.spinner("Searching clinical knowledge base..."):
        ranked_results = keyword_search(st.session_state.current_query, data, word_index)
        filtered_results = filter_results(
            ranked_results,
            st.session_state.min_score,
            st.session_state.keyword_filters,
            st.session_state.cancer_type_filter,
            st.session_state.gene_filter
        ) if ranked_results else [] # Ensure ranked_results is not empty before filtering

        if filtered_results:
            display_results(filtered_results, ranked_results)
        else:
            show_no_results(data, word_index)

def display_results(results, all_results):
    """
    Displays the filtered search results, including download options and match details.
    """
    st.success(f"Found {len(results)} relevant results (from {len(all_results)} total matches before filtering)")
    
    # Display a bar chart of scores if there are multiple results
    if len(results) > 1:
        scores = [r["score"] for r in results]
        st.bar_chart(pd.DataFrame({"Match Score": scores}), use_container_width=True)
    
    # Download buttons for all currently filtered results (JSON and CSV)
    all_results_data = [result["entry"] for result in results]
    col1, col2 = st.columns(2)
    with col1:
        st.download_button(
            "Download Filtered Results as JSON",
            json.dumps(all_results_data, indent=2),
            file_name="filtered_cancer_search_results.json",
            mime="application/json"
        )
    with col2:
        st.download_button(
            "Download Filtered Results as CSV",
            pd.DataFrame(all_results_data).to_csv(index=False),
            file_name="filtered_cancer_search_results.csv",
            mime="text/csv"
        )

    # Display each individual result in an expander
    for i, result in enumerate(results, 1):
        entry = result["entry"]
        
        # Prepare keywords for highlighting: original query words (stemmed) + actual matched words from the entry
        highlight_terms = set(stemmer.stem(word) for word in st.session_state.current_query.lower().split() if word.strip())
        highlight_terms.update(result["matched_keywords"]) # Add actual matched keywords from entry

        with st.expander(f"#{i} | Score: {result['score']:.2f} - {entry['prompt'][:70]}...", expanded=(i==1)):
            st.markdown(f"**Question:** {highlight_keywords(entry['prompt'], highlight_terms)}")
            st.markdown(f"**Answer:** {highlight_keywords(entry['completion'], highlight_terms)}")
            
            # Display metadata if available
            metadata_parts = []
            if entry["cancer_type"]:
                metadata_parts.append(f"**Cancer Type:** {entry['cancer_type']}")
            if entry["genes"]:
                metadata_parts.append(f"**Genes:** {entry['genes']}")
            if metadata_parts:
                st.markdown(" | ".join(metadata_parts))
            
            # Detailed match information in a nested expander
            with st.expander("üîç Match Details"):
                st.markdown(f"**Total Score:** {result['score']:.2f}")
                st.markdown(f"**Prompt Matches:** {result['prompt_matches']:.2f}")
                st.markdown(f"**Answer Matches:** {result['completion_matches']:.2f}")
                if result["matched_keywords"]:
                    # Display the stemmed keywords that contributed to the match
                    st.markdown(f"**Keywords Involved in Match:** `{', '.join(result['matched_keywords'])}`")
            
            # Download buttons for individual result
            col1_dl, col2_dl = st.columns(2)
            with col1_dl:
                st.download_button(
                    "Download as JSON",
                    json.dumps(entry, indent=2),
                    file_name=f"cancer_result_{i}.json",
                    mime="application/json",
                    key=f"json_single_{i}" # Unique key for each button
                )
            with col2_dl:
                st.download_button(
                    "Download as CSV",
                    pd.DataFrame([entry]).to_csv(index=False),
                    file_name=f"cancer_result_{i}.csv",
                    mime="text/csv",
                    key=f"csv_single_{i}" # Unique key for each button
                )

def show_no_results(data, word_index):
    """
    Displays a message when no results are found and provides search tips and suggestions.
    """
    st.error("üòû No matches found with current filters.")
    st.info("Try these suggestions or adjust your filters:")
    
    # Generate suggestions based on the current query's stemmed words, including fuzzy matches
    query_words_stemmed = get_clean_words(st.session_state.current_query)
    suggestions_from_query = set()

    if query_words_stemmed and word_index and data:
        potential_doc_ids = set()
        # Collect document IDs that contain any of the query words (stemmed)
        for q_word in query_words_stemmed:
            if q_word in word_index:
                potential_doc_ids.update(word_index[q_word])
        
        # Also consider fuzzy matches to suggest more relevant prompts
        all_indexed_words = list(word_index.keys())
        for q_word in query_words_stemmed:
            for indexed_word in all_indexed_words:
                if levenshtein_distance(q_word, indexed_word) <= LEVENSHTEIN_THRESHOLD:
                    # Add documents associated with fuzzy matched words
                    potential_doc_ids.update(word_index[indexed_word])
        
        # Limit suggestions to a reasonable number of unique prompts
        for doc_id in list(potential_doc_ids)[:20]: # Check up to 20 potential documents
            if doc_id < len(data):
                suggestions_from_query.add(data[doc_id]["prompt"])
                if len(suggestions_from_query) >= 5: # Show up to 5 unique suggestions
                    break

    if suggestions_from_query:
        st.write("**You might find these related questions helpful:**")
        cols = st.columns(2)
        for i, suggestion in enumerate(list(suggestions_from_query)):
            with cols[i % 2]:
                if st.button(suggestion[:70] + "..." if len(suggestion) > 70 else suggestion, 
                             key=f"nores_sugg_{i}"):
                    st.session_state.current_query = suggestion
                    st.session_state.show_home = False
                    st.rerun()
    else:
        st.write("**No immediate suggestions found based on your query.**")
        st.markdown("Try broadening your search terms or removing some filters.")

    st.markdown("---")
    st.markdown("""
    **General Search Tips:**
    - Try lowering the **Minimum Match Score** filter.
    - Remove some **Keyword**, **Cancer Type**, or **Gene** filters to broaden your search.
    - Double-check for typos in your search terms.
    - Use more general terms if your search is too specific.
    - Consider alternative phrasing for your question.
    """)

# --- Main Application Flow ---

def main():
    """Main function to run the Streamlit application."""
    st.set_page_config(
        page_title="üß¨ Cancer Clinical Trial Search",
        page_icon="üß¨",
        layout="wide", # Use wide layout for better display of results and filters
        initial_sidebar_state="expanded" # Sidebar expanded by default
    )

    # Load search history at the start of the app
    if not st.session_state.search_history:
        st.session_state.search_history = load_search_history()

    # Load data and suggestions only once or if not already loaded (cached)
    # This populates the multiselect options for filters
    if not st.session_state.suggestions or not st.session_state.cancer_types or not st.session_state.genes:
        data, word_index, cancer_types, genes, suggestions = load_and_index_data()
        if suggestions:
            st.session_state.suggestions = suggestions
        if cancer_types:
            st.session_state.cancer_types = cancer_types
        if genes:
            st.session_state.genes = genes

    # Sidebar content (always visible)
    with st.sidebar:
        st.image("https://via.placeholder.com/150x50?text=Cancer+Search+MVP", width=150) # Placeholder image
        st.title("Navigation")

        # "Home" button only appears when not on the home page
        if not st.session_state.show_home:
            if st.button("üè† Home"):
                st.session_state.show_home = True
                st.rerun()

        st.markdown("---")
        st.markdown("### About This App")
        st.markdown("""
        This **MVP (Minimum Viable Product)** allows researchers to quickly **extract and filter** information from **JSON-formatted clinical trial data**. 
        It supports:
        - **Keyword-based search** with enhanced fuzzy matching.
        - Filtering by **cancer types** and **genes**.
        - Viewing and downloading **structured data**.
        - A **search history** for easy recall of past queries.
        
        The app uses a basic text processing pipeline including **stemming** and **stop word removal** for more effective search results.
        """)

    # Main content area: switch between home and results pages
    if st.session_state.show_home:
        show_home()
    else:
        show_results()

# Entry point of the Streamlit application
if __name__ == "__main__":
    main()
